{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Walk Through Forpus\n",
    "\n",
    "[Forpus](https://severinsimmler.github.io/forpus) is a Python library for processing plain text corpora to various corpus formats. In most cases, each NLP tool uses its own idiosyncratic input format. This library helps you to convert a corpus very easy to the desired format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "import forpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCE = Path('..', 'corpus')\n",
    "FNAME_PATTERN = '{author}, {title}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Converting to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = forpus.Corpus(source=SOURCE, # the source directory\n",
    "                       target='json', # the target directory\n",
    "                       fname_pattern=FNAME_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Calling the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Checking the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mary_doc3': {'stem': 'mary_doc3', 'text': 'Mary has written the third and last document, but this is also pretty nice.\\n'}, 'peter_doc1': {'stem': 'peter_doc1', 'text': \"This is the first document. It's written by Peter. And it contains a lot of words.\\n\"}, 'paul_doc2': {'stem': 'paul_doc2', 'text': 'There is also a second document. This one is by Paul. Furthermore, this also contains a lot of tokens.\\n'}}\n"
     ]
    }
   ],
   "source": [
    "with Path('json', 'corpus.json').open('r', encoding='utf-8') as file:\n",
    "    print(json.load(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Converting to document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = forpus.Corpus(source=SOURCE, # the source directory\n",
    "                       target='matrix', # the target directory\n",
    "                       fname_pattern=FNAME_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Creating preprocessing functions\n",
    "Due to the structure of the some corpus formats, your corpus has to be tokenized. You can define a simple Regex-based tokenizer as below, or use e.g. the library [NLTK](https://www.nltk.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(document):\n",
    "    return re.compile('\\w+').findall(document.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pass even more preprocessing functions, which will be applied to the return value of your tokenizer. If you want to remove stopwords, you can use a function as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stopwords(tokens, stopwords=['the', 'and']):\n",
    "    return [token for token in tokens if token not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Calling the method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass every function as an argument of the method `to_document_term_matrix()`.\n",
    "\n",
    "We're using the class `collections.Counter` as counter, but you could write (or use) a function to e.g. normalize the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus.to_document_term_matrix(tokenizer=tokenizer,\n",
    "                               counter=Counter,\n",
    "                               drop_stopwords=drop_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Checking the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is</th>\n",
       "      <th>this</th>\n",
       "      <th>also</th>\n",
       "      <th>document</th>\n",
       "      <th>a</th>\n",
       "      <th>lot</th>\n",
       "      <th>written</th>\n",
       "      <th>contains</th>\n",
       "      <th>it</th>\n",
       "      <th>by</th>\n",
       "      <th>...</th>\n",
       "      <th>nice</th>\n",
       "      <th>pretty</th>\n",
       "      <th>third</th>\n",
       "      <th>one</th>\n",
       "      <th>furthermore</th>\n",
       "      <th>words</th>\n",
       "      <th>there</th>\n",
       "      <th>s</th>\n",
       "      <th>first</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mary_doc3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peter_doc1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paul_doc2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             is  this  also  document    a  lot  written  contains   it   by  \\\n",
       "mary_doc3   1.0   1.0   1.0       1.0  0.0  0.0      1.0       0.0  0.0  0.0   \n",
       "peter_doc1  1.0   1.0   0.0       1.0  1.0  1.0      1.0       1.0  2.0  1.0   \n",
       "paul_doc2   2.0   2.0   2.0       1.0  2.0  1.0      0.0       1.0  0.0  1.0   \n",
       "\n",
       "             ...    nice  pretty  third  one  furthermore  words  there    s  \\\n",
       "mary_doc3    ...     1.0     1.0    1.0  0.0          0.0    0.0    0.0  0.0   \n",
       "peter_doc1   ...     0.0     0.0    0.0  0.0          0.0    1.0    0.0  1.0   \n",
       "paul_doc2    ...     0.0     0.0    0.0  1.0          1.0    0.0    1.0  0.0   \n",
       "\n",
       "            first  tokens  \n",
       "mary_doc3     0.0     0.0  \n",
       "peter_doc1    1.0     0.0  \n",
       "paul_doc2     0.0     1.0  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(Path('matrix', 'corpus.matrix'), index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting to a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = forpus.Corpus(source=SOURCE, # the source directory\n",
    "                       target='graph', # the target directory\n",
    "                       fname_pattern=FNAME_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Calling the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus.to_graph(tokenizer=tokenizer,\n",
    "                counter=Counter,\n",
    "                variant='gexf', # there are more variants\n",
    "                drop_stopwords=drop_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Checking the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version='1.0' encoding='utf-8'?>\n",
      "<gexf version=\"1.2\" xmlns=\"http://www.gexf.net/1.2draft\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.w3.org/2001/XMLSchema-instance\">\n",
      "  <graph defaultedgetype=\"directed\" mode=\"static\" name=\"\">\n",
      "    <attributes class=\"edge\" mode=\"static\">\n",
      "      <attribute id=\"1\" title=\"frequency\" type=\"long\" />\n",
      "    </attributes>\n",
      "    <attributes class=\"node\" mode=\"static\">\n",
      "      <attribute id=\"0\" title=\"stem\" type=\"string\" />\n",
      "    </attributes>\n",
      "    <meta>\n",
      "      <creator>NetworkX 2.0</creator>\n",
      "      <lastmodified>16/03/2018</lastmodified>\n",
      "    </meta>\n",
      "    <nodes>\n",
      "      <node id=\"mary_doc3\" label=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"0\" value=\"mary_doc3\" />\n",
      "        </attvalues>\n",
      "      </node>\n",
      "      <node id=\"mary\" label=\"mary\" />\n",
      "      <node id=\"has\" label=\"has\" />\n",
      "      <node id=\"written\" label=\"written\" />\n",
      "      <node id=\"third\" label=\"third\" />\n",
      "      <node id=\"last\" label=\"last\" />\n",
      "      <node id=\"document\" label=\"document\" />\n",
      "      <node id=\"but\" label=\"but\" />\n",
      "      <node id=\"this\" label=\"this\" />\n",
      "      <node id=\"is\" label=\"is\" />\n",
      "      <node id=\"also\" label=\"also\" />\n",
      "      <node id=\"pretty\" label=\"pretty\" />\n",
      "      <node id=\"nice\" label=\"nice\" />\n",
      "      <node id=\"peter_doc1\" label=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"0\" value=\"peter_doc1\" />\n",
      "        </attvalues>\n",
      "      </node>\n",
      "      <node id=\"first\" label=\"first\" />\n",
      "      <node id=\"it\" label=\"it\" />\n",
      "      <node id=\"s\" label=\"s\" />\n",
      "      <node id=\"by\" label=\"by\" />\n",
      "      <node id=\"peter\" label=\"peter\" />\n",
      "      <node id=\"contains\" label=\"contains\" />\n",
      "      <node id=\"a\" label=\"a\" />\n",
      "      <node id=\"lot\" label=\"lot\" />\n",
      "      <node id=\"of\" label=\"of\" />\n",
      "      <node id=\"words\" label=\"words\" />\n",
      "      <node id=\"paul_doc2\" label=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"0\" value=\"paul_doc2\" />\n",
      "        </attvalues>\n",
      "      </node>\n",
      "      <node id=\"there\" label=\"there\" />\n",
      "      <node id=\"second\" label=\"second\" />\n",
      "      <node id=\"one\" label=\"one\" />\n",
      "      <node id=\"paul\" label=\"paul\" />\n",
      "      <node id=\"furthermore\" label=\"furthermore\" />\n",
      "      <node id=\"tokens\" label=\"tokens\" />\n",
      "    </nodes>\n",
      "    <edges>\n",
      "      <edge id=\"0\" source=\"mary\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"1\" source=\"has\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"2\" source=\"written\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"3\" source=\"written\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"4\" source=\"third\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"5\" source=\"last\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"6\" source=\"document\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"7\" source=\"document\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"8\" source=\"document\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"9\" source=\"but\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"10\" source=\"this\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"11\" source=\"this\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"12\" source=\"this\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"2\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"13\" source=\"is\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"14\" source=\"is\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"15\" source=\"is\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"2\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"16\" source=\"also\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"17\" source=\"also\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"2\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"18\" source=\"pretty\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"19\" source=\"nice\" target=\"mary_doc3\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"20\" source=\"first\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"21\" source=\"it\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"2\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"22\" source=\"s\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"23\" source=\"by\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"24\" source=\"by\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"25\" source=\"peter\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"26\" source=\"contains\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"27\" source=\"contains\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"28\" source=\"a\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"29\" source=\"a\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"2\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"30\" source=\"lot\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"31\" source=\"lot\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"32\" source=\"of\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"33\" source=\"of\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"34\" source=\"words\" target=\"peter_doc1\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"35\" source=\"there\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"36\" source=\"second\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"37\" source=\"one\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"38\" source=\"paul\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"39\" source=\"furthermore\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "      <edge id=\"40\" source=\"tokens\" target=\"paul_doc2\">\n",
      "        <attvalues>\n",
      "          <attvalue for=\"1\" value=\"1\" />\n",
      "        </attvalues>\n",
      "      </edge>\n",
      "    </edges>\n",
      "  </graph>\n",
      "</gexf>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with Path('graph', 'corpus.gexf').open('r', encoding='utf-8') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Converting to LDA-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = forpus.Corpus(source=SOURCE,\n",
    "                       target='ldac',\n",
    "                       fname_pattern=FNAME_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Calling the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus.to_ldac(tokenizer=tokenizer,\n",
    "               counter=Counter,\n",
    "               drop_stopwords=drop_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Checking the output\n",
    "\n",
    "Three files have been generated:\n",
    "\n",
    "1. `corpus.ldac` contains one document per line as described by [David Blei](https://github.com/blei-lab/lda-c/blob/master/readme.txt).\n",
    "2. `corpus.tokens` contains one type per line of the vocabulary. The line index is the type index.\n",
    "3. `corpus.metadata` contains the metadata extracted from filenames. This is a simple CSV-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.ldac:\n",
      "12 0:1 1:1 2:1 3:1 4:1 5:1 6:1 7:1 8:1 9:1 10:1 11:1\n",
      "14 7:1 8:1 12:1 5:1 13:2 14:1 2:1 15:1 16:1 17:1 18:1 19:1 20:1 21:1\n",
      "15 22:1 8:2 9:2 18:2 23:1 5:1 7:2 24:1 15:1 25:1 26:1 17:1 19:1 20:1 27:1\n",
      "\n",
      "\n",
      "corpus.tokens:\n",
      "mary\n",
      "has\n",
      "written\n",
      "third\n",
      "last\n",
      "document\n",
      "but\n",
      "this\n",
      "is\n",
      "also\n",
      "pretty\n",
      "nice\n",
      "first\n",
      "it\n",
      "s\n",
      "by\n",
      "peter\n",
      "contains\n",
      "a\n",
      "lot\n",
      "of\n",
      "words\n",
      "there\n",
      "second\n",
      "one\n",
      "paul\n",
      "furthermore\n",
      "tokens\n",
      "\n",
      "                                stem    basename\n",
      "../corpus/mary_doc3.txt    mary_doc3   mary_doc3\n",
      "../corpus/peter_doc1.txt  peter_doc1  peter_doc1\n",
      "../corpus/paul_doc2.txt    paul_doc2   paul_doc2\n"
     ]
    }
   ],
   "source": [
    "with Path('ldac', 'corpus.ldac').open('r', encoding='utf-8') as file:\n",
    "    print('corpus.ldac:\\n{0}\\n'.format(file.read()))\n",
    "\n",
    "with Path('ldac', 'corpus.tokens').open('r', encoding='utf-8') as file:\n",
    "    print('corpus.tokens:\\n{0}\\n'.format(file.read()))\n",
    "\n",
    "print(pd.read_csv(Path('ldac', 'corpus.metadata'), index_col=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Converting to SVMlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus = forpus.Corpus(source=SOURCE,\n",
    "                       target='svmlight',\n",
    "                       fname_pattern=FNAME_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Calling the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Corpus.to_svmlight(tokenizer=tokenizer,\n",
    "                   classes=[0 for n in range(3)], # each document belongs to class '0'\n",
    "                   counter=Counter,\n",
    "                   drop_stopwords=drop_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Checking the output\n",
    "\n",
    "Three files have been generated:\n",
    "\n",
    "1. `corpus.svmlight` contains one document per line as described by [Thorsten Joachims](http://svmlight.joachims.org/).\n",
    "2. `corpus.tokens` contains one type per line of the vocabulary. The line index is the type index.\n",
    "3. `corpus.metadata` contains the metadata extracted from filenames. This is a simple CSV-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus.svmlight:\n",
      "0 1:1 2:1 3:1 4:1 5:1 6:1 7:1 8:1 9:1 10:1 11:1 12:1\n",
      "0 8:1 9:1 13:1 6:1 14:2 15:1 3:1 16:1 17:1 18:1 19:1 20:1 21:1 22:1\n",
      "0 23:1 9:2 10:2 19:2 24:1 6:1 8:2 25:1 16:1 26:1 27:1 18:1 20:1 21:1 28:1\n",
      "\n",
      "\n",
      "corpus.tokens:\n",
      "mary\n",
      "has\n",
      "written\n",
      "third\n",
      "last\n",
      "document\n",
      "but\n",
      "this\n",
      "is\n",
      "also\n",
      "pretty\n",
      "nice\n",
      "first\n",
      "it\n",
      "s\n",
      "by\n",
      "peter\n",
      "contains\n",
      "a\n",
      "lot\n",
      "of\n",
      "words\n",
      "there\n",
      "second\n",
      "one\n",
      "paul\n",
      "furthermore\n",
      "tokens\n",
      "\n",
      "                                stem    basename\n",
      "../corpus/mary_doc3.txt    mary_doc3   mary_doc3\n",
      "../corpus/peter_doc1.txt  peter_doc1  peter_doc1\n",
      "../corpus/paul_doc2.txt    paul_doc2   paul_doc2\n"
     ]
    }
   ],
   "source": [
    "with Path('svmlight', 'corpus.svmlight').open('r', encoding='utf-8') as file:\n",
    "    print('corpus.svmlight:\\n{0}\\n'.format(file.read()))\n",
    "\n",
    "with Path('svmlight', 'corpus.tokens').open('r', encoding='utf-8') as file:\n",
    "    print('corpus.tokens:\\n{0}\\n'.format(file.read()))\n",
    "\n",
    "print(pd.read_csv(Path('svmlight', 'corpus.metadata'), index_col=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
